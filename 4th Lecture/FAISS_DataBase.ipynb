{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dcbd8a7",
   "metadata": {},
   "source": [
    "# Fiass_Database or DB : Facebook AI Similarity Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c78153",
   "metadata": {},
   "source": [
    "# Install environment : create an enviornment or env\n",
    "\n",
    "- it will craete directory. or it simply create the folder\n",
    "- Below step I can follow : Below code copy and paste inside terminal and hit enter.\n",
    "- here -> (-n) means name of folder, I given name as \"vectordb\"\n",
    "\n",
    "- code :  [ conda create -n vectordb python=3.10 ] \n",
    "- Now, activate the env -> code : [ conda activate vectordb ] \n",
    "- Here means -> conda activate & give the folder name or env name -> \"vectordb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f22a1c7",
   "metadata": {},
   "source": [
    "# Install libaries :\n",
    "\n",
    "- Dont run here, it will not work my case, open the terminal & run inside it, its working copy and paste these below libaries.\n",
    "\n",
    "- 1.pip install ipykernel\n",
    "- 2.python -m ipykernel --version  <- this is basically to check the version\n",
    "- 3.import faiss <- import the Faiss library, which stands for Facebook AI Similarity Search.\n",
    "- 4.pip install faiss-cpu \n",
    "- 5.pip install numpy\n",
    "- 6.pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b729d9",
   "metadata": {},
   "source": [
    "# Note : Just for understanding !\n",
    "- Select the Kernel inside VS code\n",
    "- 1.If I will pick -> venv/python.exe <- this is my present directory if I will select this then, it will not the \"ipykernel\".\n",
    "\n",
    "- 2.If I will pick these both any one option -> \"conda\\envs\\venv\\python.exe\" | \"base\\conda\" then both option works, because its Global.\n",
    "- So, I will pick the -> venv( python 3.18.10 )\\\"conda\\envs\\venv\\python.exe\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89e6247e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\vijay\\.conda\\envs\\venv\\lib\\site-packages (2.2.6)\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a2e5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ipykernel in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (6.30.1)\n",
      "Requirement already satisfied: comm>=0.1.1 in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel) (0.2.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel) (1.8.17)\n",
      "Requirement already satisfied: ipython>=7.23.1 in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel) (8.37.0)\n",
      "Requirement already satisfied: jupyter-client>=8.0.0 in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel) (8.6.3)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel) (5.8.1)\n",
      "Requirement already satisfied: matplotlib-inline>=0.1 in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel) (0.1.7)\n",
      "Requirement already satisfied: nest-asyncio>=1.4 in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel) (1.6.0)\n",
      "Requirement already satisfied: packaging>=22 in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel) (25.0)\n",
      "Requirement already satisfied: psutil>=5.7 in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel) (7.1.0)\n",
      "Requirement already satisfied: pyzmq>=25 in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel) (27.1.0)\n",
      "Requirement already satisfied: tornado>=6.2 in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel) (6.5.2)\n",
      "Requirement already satisfied: traitlets>=5.4.0 in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from ipykernel) (5.14.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=7.23.1->ipykernel) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=7.23.1->ipykernel) (5.2.1)\n",
      "Requirement already satisfied: exceptiongroup in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=7.23.1->ipykernel) (1.3.0)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=7.23.1->ipykernel) (0.19.2)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=7.23.1->ipykernel) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=7.23.1->ipykernel) (2.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=7.23.1->ipykernel) (0.6.3)\n",
      "Requirement already satisfied: typing_extensions>=4.6 in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from ipython>=7.23.1->ipykernel) (4.15.0)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython>=7.23.1->ipykernel) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from jedi>=0.16->ipython>=7.23.1->ipykernel) (0.8.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-client>=8.0.0->ipykernel) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (4.5.0)\n",
      "Requirement already satisfied: pywin32>=300 in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel) (311)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->jupyter-client>=8.0.0->ipykernel) (1.17.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from stack_data->ipython>=7.23.1->ipykernel) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from stack_data->ipython>=7.23.1->ipykernel) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\vijay\\appdata\\roaming\\python\\python310\\site-packages (from stack_data->ipython>=7.23.1->ipykernel) (0.2.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install ipykernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c1eb9d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e06895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpu\n",
      "  Downloading faiss_cpu-1.12.0-cp310-cp310-win_amd64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\vijay\\.conda\\envs\\venv\\lib\\site-packages (from faiss-cpu) (2.2.6)\n",
      "Requirement already satisfied: packaging in c:\\users\\vijay\\.conda\\envs\\venv\\lib\\site-packages (from faiss-cpu) (25.0)\n",
      "Downloading faiss_cpu-1.12.0-cp310-cp310-win_amd64.whl (18.2 MB)\n",
      "   ---------------------------------------- 0.0/18.2 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/18.2 MB 3.4 MB/s eta 0:00:06\n",
      "   -- ------------------------------------- 1.3/18.2 MB 3.4 MB/s eta 0:00:06\n",
      "   ----- ---------------------------------- 2.6/18.2 MB 4.6 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 4.2/18.2 MB 5.6 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 6.3/18.2 MB 6.3 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 7.9/18.2 MB 6.6 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 9.2/18.2 MB 6.6 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 10.5/18.2 MB 6.5 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 11.5/18.2 MB 6.3 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 12.6/18.2 MB 6.2 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 13.9/18.2 MB 6.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 14.9/18.2 MB 6.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 16.0/18.2 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 17.0/18.2 MB 5.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 17.6/18.2 MB 5.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.2/18.2 MB 5.5 MB/s  0:00:03\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df34c713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.4.3-cp310-cp310-win_amd64.whl.metadata (37 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
      "Using cached requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.3-cp310-cp310-win_amd64.whl (107 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
      "Installing collected packages: urllib3, idna, charset_normalizer, certifi, requests\n",
      "\n",
      "   ---------------------------------------- 0/5 [urllib3]\n",
      "   ---------------------------------------- 0/5 [urllib3]\n",
      "   ---------------------------------------- 0/5 [urllib3]\n",
      "   ---------------------------------------- 0/5 [urllib3]\n",
      "   ---------------------------------------- 0/5 [urllib3]\n",
      "   ---------------------------------------- 0/5 [urllib3]\n",
      "   -------- ------------------------------- 1/5 [idna]\n",
      "   -------- ------------------------------- 1/5 [idna]\n",
      "   ---------------- ----------------------- 2/5 [charset_normalizer]\n",
      "   ---------------- ----------------------- 2/5 [charset_normalizer]\n",
      "   ---------------- ----------------------- 2/5 [charset_normalizer]\n",
      "   ---------------- ----------------------- 2/5 [charset_normalizer]\n",
      "   ------------------------ --------------- 3/5 [certifi]\n",
      "   -------------------------------- ------- 4/5 [requests]\n",
      "   -------------------------------- ------- 4/5 [requests]\n",
      "   -------------------------------- ------- 4/5 [requests]\n",
      "   ---------------------------------------- 5/5 [requests]\n",
      "\n",
      "Successfully installed certifi-2025.10.5 charset_normalizer-3.4.3 idna-3.10 requests-2.32.5 urllib3-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9b1bf2",
   "metadata": {},
   "source": [
    "# Note :\n",
    "\n",
    "- Now everything is installed, now we need a data, we are going to perform some sort of Embedding Operation. \n",
    "- \"Embedding Operation\" - means we are going to take some textual information, and eventually we will try to convert those textual \n",
    "- information, into a numerical vector. After doing that so, we are going to store those information into a databases.\n",
    "- This is something we will going do over here. \n",
    "- We need a dataset, I can get the data from the multiple sources. \n",
    "- Again in realtime, people will going to give you the data again from multiple different different places, there is possibility that,\n",
    "\n",
    "- people will going to provide the data into a\n",
    "- 1.PDF format,\n",
    "- 2.Excel sheet,\n",
    "- 3.URL - & asked you to scrapped those data, may be data will be avaiable in some SQL or NoSQL databases.\n",
    "\n",
    "- There is alot of possibiity that, dataset will be avaiable in different diffent location."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70ecbb2",
   "metadata": {},
   "source": [
    "# Next :\n",
    "\n",
    "- As of now we will discussed the Conversion the data and storage part, but in general in realtime,\n",
    "- we try to build a complete Datapipeline. So, where we try to read data from the multiple sources. There is possibility that I am going \n",
    "- to received the data into a batch mode. There is possibility that I will be received a data into a \"streaming mode\" & there is also a \n",
    "- possibility that I might received a data into a \"mini batch mode\", means I am talking about the frequency of the data.\n",
    "- There is the possibility & there is possibility that data is coming from multiple different different sources. Maybe from the \n",
    "\n",
    "- 1.S3 Bucket, \n",
    "- 2.Azure Blobs \n",
    "- or maybe from the databases, there are 100 of different different databases, whihc is avaiable maybe from the different different\n",
    "- file system.\n",
    "\n",
    "- As of now I am going to take the data from this page -> Steps : \n",
    "- Go to \"Euron\", next go to \"Founder story\" URL : https://euron.one/our-founder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c1e801",
   "metadata": {},
   "source": [
    "# Next :\n",
    "\n",
    "- Lets come this page and copy both paragram from this : \"Making an Impact\".  URL : https://euron.one/our-founder\n",
    "\n",
    "- 1.Helping Millions of Students Succeed & <- Copy the paragraph \n",
    "- 2.The Entrepreneur and Teacher: Sudhanshu's Dual Legacy <- Copy the paragraph \n",
    "\n",
    "- So, lets create the \"vaiable = data\" &  Paste the data, that I have copied from the these 2 paragraph.\n",
    "- so, this is the data that,I am copy and paste from the website.\n",
    "- Also there is possibility that we will received a data from the PDF format. In that case we have to write the Python Function to read\n",
    "- out those data from a PDF file. Maybe someone going to give the data into a \n",
    "\n",
    "- 1.Word format\n",
    "- 2.Excel sheet format\n",
    "- 3.( .Txt format ) \n",
    "- 4.JSON format\n",
    "\n",
    "- So, there can be thousand of different different kind of sources of the data, that we will be able to find out, in a realtime."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea325820",
   "metadata": {},
   "source": [
    "# Next : \n",
    "\n",
    "- If we see this below data, then we can observed that, there is different different lines or paragraphs. \n",
    "- So, my very 1st approach should be to break down, this entire dataset into a small, small chunks. \n",
    "- For example : We are reading a PDF, which is 100 of pages somewhere around, so its not like I will take one single variable and I will\n",
    "- try to keep all those hundreds of pages, and then I will try to convert those 100 of pages, all together in one single vector.\n",
    "- Because that is not going to work for me, Because the whole idea behind doing \"Embedding\" is to improvise our Search. \n",
    "- Improvise our Indexing, that is not going to help.\n",
    "- So, what I am suppose to do next, If I have this kind of dataset,\n",
    "- then basically I will try to break this data, into a small, small chunks of size may be 500 text or maybe 1,000 of texts. Or maybe \n",
    "- thousand characters, or maybe thousand of words, depends.\n",
    "- But, Yes I am going to break down this into a small, small chunks, by keeping some sort of a overlap between a chunks and\n",
    "- why we are going to maintain that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1614b0d0",
   "metadata": {},
   "source": [
    "# Let's talk about this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce28326d",
   "metadata": {},
   "source": [
    "- So we are going to take this below data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1a99f025",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = \"\"\" \n",
    "Making an Impact\n",
    "Helping Millions of Students Succeed\n",
    "Sudhanshu's commitment to affordable education wasn't just a business strategy—it was his life's mission. Over the years, iNeuron has helped over 1.5 million students from 34+ countries, providing them with the skills they need to succeed in today's competitive job market.\n",
    "Many of these students, like Sudhanshu himself, came from disadvantaged backgrounds. They saw iNeuron as a lifeline—an opportunity to rise above their circumstances.\n",
    "In 2022, iNeuron was acquired by PhysicsWallah in a deal worth ₹250 crore. While this acquisition was a significant milestone, Sudhanshu remained focused on his mission. Even after the acquisition, iNeuron continued to offer some of the most affordable and accessible tech courses in the world.\n",
    "\n",
    "The Entrepreneur and Teacher: Sudhanshu's Dual Legacy\n",
    "Sudhanshu's journey isn't just one of entrepreneurial success; it's also a story of dedication to teaching. Throughout his career, he has remained a passionate educator, constantly looking for ways to empower others through knowledge. Whether teaching courses in Big Data,\n",
    "Data Science, or programming, Sudhanshu has always sought to make complex subjects accessible to learners at all levels.\n",
    "\n",
    "His commitment to affordable education has earned him the respect and admiration of countless students. Many credit Sudhanshu with changing their lives, helping them secure jobs, improve their skills, and break free from the limitations of their backgrounds.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fe0c36",
   "metadata": {},
   "source": [
    "# Next :\n",
    "\n",
    "- Now here we are going to call a -> \"data.strip()\" <- This will eliminate the all those leading and trailing spaces inbetween our data. \n",
    "- Let me name it as a \"Clean_data\" a Variable name.\n",
    "- There so many different types of clearning we can try to perform, this is just a symbolic one.\n",
    "- We can perfrom a Normilization operation, we can just try to remove the pencutations, or any kind of unncessary most of the time \n",
    "- a repetitive words. So, many differen different type kind of NLP based cleaning we can try to perform on top of data, now but again that\n",
    "- depends upon the kind of problem statement, which I am going to solve. So that depends more like on a business domain, or like problem\n",
    "- statement that we are going to solve.\n",
    "- So, as of now just \"Symbolic\" manner so I have called \"data strip\", so which going to remove leading & trailling spaces from the dataset.\n",
    "- I am calling that dataset as a clean data lets suppose.\n",
    "- But again, on top of this we do a lot of processing with respect to our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4412156e",
   "metadata": {},
   "source": [
    "# Next : \n",
    "\n",
    "- Let's suppose, I a trying to read a PDF, there will be so many different different kind of edge case, getting & I have to handle, all\n",
    "- of the edge cases, then only I will be able to get the required dataset, but here dataset that we are using is not much complicated,\n",
    "- There is a possibility, that I will be having some of the word or some of the line inside my dataset into a different language,\n",
    "- May be Into a \"Spanish\", \"Franch\" or \"Hindi\" language, there will be very high chance, when I am trying to read some sort of a books,\n",
    "- or maybe a PDF, or some sort of a data, there is chance that I am end of getting, mixture of a language, which we say multi lanuage.\n",
    "- there is a very high possibility, I have to handle all of those situation, with respect to a dataset, then only I can try to make my  \n",
    "- dataset ideal or any kind of operation, whether its a embedding or maybe whether a \"RAG\" or any other agentic operation. \n",
    "- alots of thing is goes around interms of building a complete data pipeline, again that is major roles and responsibility we all try to\n",
    "- inside a organization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad7d2a6",
   "metadata": {},
   "source": [
    "- So, here I am able to read a data in a Symbolic manner, I am able to call a \"data.Strip()\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f3a2584",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data = data.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8c7fb16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Making an Impact\\nHelping Millions of Students Succeed\\nSudhanshu's commitment to affordable education wasn't just a business strategy—it was his life's mission. Over the years, iNeuron has helped over 1.5 million students from 34+ countries, providing them with the skills they need to succeed in today's competitive job market.\\nMany of these students, like Sudhanshu himself, came from disadvantaged backgrounds. They saw iNeuron as a lifeline—an opportunity to rise above their circumstances.\\nIn 2022, iNeuron was acquired by PhysicsWallah in a deal worth ₹250 crore. While this acquisition was a significant milestone, Sudhanshu remained focused on his mission. Even after the acquisition, iNeuron continued to offer some of the most affordable and accessible tech courses in the world.\\n\\nThe Entrepreneur and Teacher: Sudhanshu's Dual Legacy\\nSudhanshu's journey isn't just one of entrepreneurial success; it's also a story of dedication to teaching. Throughout his career, he has remained a passionate educator, constantly looking for ways to empower others through knowledge. Whether teaching courses in Big Data,\\nData Science, or programming, Sudhanshu has always sought to make complex subjects accessible to learners at all levels.\\n\\nHis commitment to affordable education has earned him the respect and admiration of countless students. Many credit Sudhanshu with changing their lives, helping them secure jobs, improve their skills, and break free from the limitations of their backgrounds.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a31585",
   "metadata": {},
   "source": [
    "# What is the total length of my string or paragraph?\n",
    "\n",
    "- As a outcome, we can see its a -> ( 1,497 ).\n",
    "- This is basically total, like a space which my dataset has occupied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dc1a369",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1497"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(clean_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc1c079",
   "metadata": {},
   "source": [
    "# Next : \n",
    "\n",
    "- Now my requirement is breakdown this dataset, into a small small chunks, means \n",
    "- For example :\n",
    "- 1.One single line, I would like to keep maybe 800 character, that is one chunks.\n",
    "- 2.Another 800 character this is again another chunk.\n",
    "- 3.Another 800 character this is again another chunk.\n",
    "\n",
    "- Like this I can try to create a chunks, with certain chunks size, so in one single sentences, or one single data, having only that much\n",
    "- number of characters or may be I can try to define it, based on the words, whatever suits us. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1764a40",
   "metadata": {},
   "source": [
    "# Next: \n",
    "\n",
    "- Let me define some of the variables :\n",
    "\n",
    "- Max_char or maximum character = 800, means in one single line there will be a 800 character max to max, not more then that.\n",
    "- Then I can try to define \"Overlapp\" = 100, means lets suppose I have line No.1 & I have line No.2, there is a overlapped of 100 character\n",
    "- between the line No.1 and line No.2. \n",
    "\n",
    "- Code : [ overlap = 100 ]\n",
    "- Why I am trying to keep the overlapp here ?\n",
    "- Because when I am going to do a chunking operation, when I am going to breakdown this data or paragraph into a chunks.\n",
    "- So, I have to maintain even a relation, means context. If there will be an overlapped, between a data, onviously it will be able to see\n",
    "- the contect, it will be able to correlate.  And when I am going to do search operation, on the top of the embedding, it will be able to\n",
    "- give me like, there is some match between line No.1 and there is some match between Line No.2 as well. Just to maintain that context,\n",
    "- I am going to maintain the overlapping, between 2 datasets. That is the region, we have given overlapped, with hundreds of like\n",
    "- a characters.\n",
    "\n",
    "- Code : [ Chunk = []  ] <- Then try to create a chunks as a \"List\"\n",
    "- Then I will try to keep [i] variable as a [0] zero. means start from [0] indexes. Here -> \"Making an Impact\" in paragraph we can see.\n",
    "\n",
    "- Code : [ While i <len(clean_data): ]\n",
    "- while [i] is lesser (<) than, length of a (clean_data), start from [0] index go till last one, this is the region lenth of (Clean_data)\n",
    "- If I have 500 word or more then that in my pagagraph it will go till last, then only it will going to break the loop,this is what I mean\n",
    "- While i <len(clean_data): <- when this statement is true, then it has to perform different different operation, \n",
    "\n",
    "- What kind of operation, it has to perform? \n",
    "- it has to break down my entire dataset into a small, small pieces.\n",
    "\n",
    "- Code : piece = clean_data[i:i+max_char] \n",
    "- Lets suppose, here I am going to call my \"Clean_data\" , \"Clean_data\" is my variable name by the way, my whole paragraph is inside\n",
    "- this variable.\n",
    "- \"Clean_data\"[] <- inside this I am going to write the index, like start from here and then go till this particular point.\n",
    "- So, I am performing slicing opeartion, I am saying start from [i] means start from [0] index, go till [i] + maximum character which is\n",
    "- our 800. means [0+800]. this -> clean_data[i:i+max_char] whole will going to be one data.\n",
    "- Now, lets say I will keep this data inside varaiable -> (\"Piece\"). This -> clean_data[i:i+max_char] <- will be having in a very \n",
    "- 1st interation, it will be having a data (i = 0th) index till 800, this is the maximum character which I have given.\n",
    "- This is the line No.1 which I am going to maintain,  \n",
    "- \n",
    "- Code : chunks.append(piece)\n",
    "- Here, I am going to append \"chunks\", what ? append (\"piece\"). this is the list -> [ Chunks = [] ] <- which I have created.\n",
    "- So, I am going to append this -> Code : piece = clean_data[i:i+max_char]  & chunks.append(piece) <- particular piece inside this\n",
    "- particulat list -> [ chunks = [] ] <-\n",
    "\n",
    "- Code : i = i+max_char - overlap \n",
    "\n",
    "- Now we will going to increment the value of [i], we have to shift the value of [i] now.\n",
    "\n",
    "- Now [i] is = [i]+max_char-overlap  \n",
    "- Now, I am going to reset my value of [i], where will be my [i] by the way?\n",
    "- so, I has started from the [0] zero, it went till the 800, now my [i] is avaiable in the 800th location.\n",
    "- But, I have to set the value of this [i] at the (700) location.why?\n",
    "- So that, I will be able to maintain a overlap, of 100 inbetween. Between from the 1st sentences & between the 2nd sentences. \n",
    "- That is the region, I have written the (i+max_char) means ( 0+800 ) - 100. which means the 700, overlap.\n",
    "- SO, now my [i] is here -> in 700.  [ 0,1,2,3,4,5,6.........700,701,702,703......800 ]\n",
    "- so, next time, when [i] is going to start from where? this line code[i], I am talking about here -> while i<len(clean_data):\n",
    "- Next time, [i] will going to start from the 700 by the way, [700, 7001, 7002......800 ], then (700+800) = 1500 technically.\n",
    "- so, it will go till the (1500). \n",
    "- Again, I will try to reset the value of [i] at (1400). then again I will going to start from (1400), then I will be able to maintain\n",
    "- overlapped of 100, 100, 1000 inbetween. \n",
    "- Here, I am playing the value of [i], with those [i] those indexes, I am able to maintain the overlapped.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cb28202",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_char = 800\n",
    "overlap = 100\n",
    "chunks = []\n",
    "i = 0\n",
    "while i <len(clean_data):\n",
    "    piece = clean_data[i:i+max_char]\n",
    "    chunks.append(piece)\n",
    "    i = i+max_char - overlap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76ff5f1",
   "metadata": {},
   "source": [
    "# Why do we need to maintain the overlapped?\n",
    "- we need to maintain the overlapped, so that we will be maintain the context, a little bit of context we will be able maintain in the \n",
    "- both the sentences.\n",
    "- The total is 1497, like a space which my dataset has occupied. so max to max I am able to creating 2 chunks. \n",
    "- Inside the chunks, I will be having the max to max like 2 sentences or 3 sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b280b8ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Making an Impact\\nHelping Millions of Students Succeed\\nSudhanshu's commitment to affordable education wasn't just a business strategy—it was his life's mission. Over the years, iNeuron has helped over 1.5 million students from 34+ countries, providing them with the skills they need to succeed in today's competitive job market.\\nMany of these students, like Sudhanshu himself, came from disadvantaged backgrounds. They saw iNeuron as a lifeline—an opportunity to rise above their circumstances.\\nIn 2022, iNeuron was acquired by PhysicsWallah in a deal worth ₹250 crore. While this acquisition was a significant milestone, Sudhanshu remained focused on his mission. Even after the acquisition, iNeuron continued to offer some of the most affordable and accessible tech courses in the world.\\n\\nThe Entrep\",\n",
       " \"continued to offer some of the most affordable and accessible tech courses in the world.\\n\\nThe Entrepreneur and Teacher: Sudhanshu's Dual Legacy\\nSudhanshu's journey isn't just one of entrepreneurial success; it's also a story of dedication to teaching. Throughout his career, he has remained a passionate educator, constantly looking for ways to empower others through knowledge. Whether teaching courses in Big Data,\\nData Science, or programming, Sudhanshu has always sought to make complex subjects accessible to learners at all levels.\\n\\nHis commitment to affordable education has earned him the respect and admiration of countless students. Many credit Sudhanshu with changing their lives, helping them secure jobs, improve their skills, and break free from the limitations of their backgrounds.\",\n",
       " 'them secure jobs, improve their skills, and break free from the limitations of their backgrounds.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476515dc",
   "metadata": {},
   "source": [
    "# check Lenth or (len)\n",
    "\n",
    "- Here, If I will going to check his length. \n",
    "- As we can see its a 3 basically. So, max to max 3 sentences I am able to form, not more then that.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0ce9589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813d8198",
   "metadata": {},
   "source": [
    "# Next : [ max_char = 300 ]\n",
    "\n",
    "- Lets say, I am taking or maintain as [ max_char = 300 ].\n",
    "- As a outcome, we can see over here, I am able to create 8 lines, out of my dataset & all of these lines are avaiable inside this\n",
    "- particular list[].\n",
    "\n",
    "- If we observed here, in our outcome, we can see some symbols, (\\), like this we have to clean in the cleaing operation, because these\n",
    "- will not going to give me value, with respect to a data. We have to perform some claning opeartion on top of this.\n",
    "- There maybe some numerical value which is not of our use.\n",
    "- Maybe our PDF data is avaiable in some different language, so then again we have to go with the cleaning operation. \n",
    "- There are alot of \"Data preprocessing\" operation, we try to perform, which is true with across all the application.\n",
    "- Not just with this application, whether we are trying to build a simple \"Machine Learning\" model or \"Deep Learning\" model.\n",
    "- or we are dealing with some sort of \"Data analytics\" everywhere, we have to go through this kind of process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fc4742",
   "metadata": {},
   "source": [
    "# Next, : Overlapped\n",
    "\n",
    "- In this dataset, there are overlapped also, we we cross check here, if we go end of this data \n",
    "- -> this 1st line inside outcome, we can see -> \"skills they need to succeed in toda \". (\"toda\") <- this only it is able to capture \n",
    "- Here, we will be able to see that 2nd paragraph of this outcome -> [\"skills they need to succeed in today's competitive job market\"] \n",
    "- Here, we can see full name -> Today's not \"toda\".\n",
    "- There is overlapped, I am trying to maintain between 2 sentences, so that it will be able to maintain a contextual information.\n",
    "- It will be able to understand something, like this 2 lines are connected to each others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2193a06",
   "metadata": {},
   "source": [
    "# Next :\n",
    "\n",
    "- Now, we are able to understand how we are able to create an chunks, simple Python function.\n",
    "- There are inbuild libaries also there, which we can even try to use that one. Moreless this is something people are doing currently.\n",
    "- even insid those libaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85534f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_char = 300\n",
    "overlap = 100\n",
    "chunks = []\n",
    "i = 0\n",
    "while i <len(clean_data):\n",
    "    piece = clean_data[i:i+max_char]\n",
    "    chunks.append(piece)\n",
    "    i = i+max_char - overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5ba1854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Making an Impact\\nHelping Millions of Students Succeed\\nSudhanshu's commitment to affordable education wasn't just a business strategy—it was his life's mission. Over the years, iNeuron has helped over 1.5 million students from 34+ countries, providing them with the skills they need to succeed in toda\",\n",
       " \"1.5 million students from 34+ countries, providing them with the skills they need to succeed in today's competitive job market.\\nMany of these students, like Sudhanshu himself, came from disadvantaged backgrounds. They saw iNeuron as a lifeline—an opportunity to rise above their circumstances.\\nIn 202\",\n",
       " 'backgrounds. They saw iNeuron as a lifeline—an opportunity to rise above their circumstances.\\nIn 2022, iNeuron was acquired by PhysicsWallah in a deal worth ₹250 crore. While this acquisition was a significant milestone, Sudhanshu remained focused on his mission. Even after the acquisition, iNeuron ',\n",
       " \"gnificant milestone, Sudhanshu remained focused on his mission. Even after the acquisition, iNeuron continued to offer some of the most affordable and accessible tech courses in the world.\\n\\nThe Entrepreneur and Teacher: Sudhanshu's Dual Legacy\\nSudhanshu's journey isn't just one of entrepreneurial su\",\n",
       " \"reneur and Teacher: Sudhanshu's Dual Legacy\\nSudhanshu's journey isn't just one of entrepreneurial success; it's also a story of dedication to teaching. Throughout his career, he has remained a passionate educator, constantly looking for ways to empower others through knowledge. Whether teaching cour\",\n",
       " 'ate educator, constantly looking for ways to empower others through knowledge. Whether teaching courses in Big Data,\\nData Science, or programming, Sudhanshu has always sought to make complex subjects accessible to learners at all levels.\\n\\nHis commitment to affordable education has earned him the res',\n",
       " 'accessible to learners at all levels.\\n\\nHis commitment to affordable education has earned him the respect and admiration of countless students. Many credit Sudhanshu with changing their lives, helping them secure jobs, improve their skills, and break free from the limitations of their backgrounds.',\n",
       " 'them secure jobs, improve their skills, and break free from the limitations of their backgrounds.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "12a55367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d54d4c",
   "metadata": {},
   "source": [
    "# Note : Convert this entire dataset into a Embedding.\n",
    "\n",
    "- Convert this entire dataset into a Embedding, into a numerical format which we have to convert this data.\n",
    "- We done last class, we can use \"EuriAPI\", we can use some models from the \"Hugging Face\" or any open sources model, which is going \n",
    "- to support a embeddding and I will be able to convert my dataset into a vector. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edfd922c",
   "metadata": {},
   "source": [
    "- Let's try to convert this entire dataset into a embedding, into a vector spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9989bf2",
   "metadata": {},
   "source": [
    "# Note : Follow the steps : Just copy and paste dn't run below code.\n",
    "\n",
    "- Go to the \"EuriAPI\" -> Code Example -> Text Embedding Similarity <- Just copy the code from here. This is basically for \"Text Embedding Similarity\"\n",
    "\n",
    "- Just copy and paste the code, dont run it, we need to replace with \"API keys\" and \"Authorization\". \n",
    "\n",
    "- Without \"API keys\" we cant use this piece of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83803a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import requests\n",
    "#import numpy as np\n",
    "\n",
    "#def generate_embeddings(text):\n",
    "    url = \"https://api.euron.one/api/v1/euri/embeddings\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer ********************\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"input\": text,\n",
    "        \"model\": \"text-embedding-3-small\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    data = response.json()\n",
    "    \n",
    "    embedding = np.array(data['data'][0]['embedding'])\n",
    "    \n",
    "    return embedding\n",
    "\n",
    "#text = \"The weather is sunny today.\"\n",
    "\n",
    "#embedding = generate_embeddings(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd54a81",
   "metadata": {},
   "source": [
    "# Lets start :\n",
    "\n",
    "- To get API key : Follow steps : Go to \"EURI\" and -> [\"API keys\"] -> Name it & create it. I named it as -> [\"Fiass_Vector_DB API\"]\n",
    "- This is my API Key :  [   euri-78c66f791ec6c9b285bc2f92d2487d5c61f0bec3c10c01a8f89fb20aba1c5ae1   ]\n",
    "\n",
    "- Just replace this API key here -> [ \"Authorization\": \"Bearer ********************\" ]-< "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc35b7b",
   "metadata": {},
   "source": [
    "# Next :\n",
    "\n",
    "- This is the function, -> [def generate_embeddings(text): ] <- which will going to take the data, a text and it is going \n",
    "- give the Embedding. It is going to take the data and it is going to give the Embedding.\n",
    "- we replace the API, we will clean and remove last 2 lines code & just execute the below code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd32d0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "def generate_embeddings(text):\n",
    "    url = \"https://api.euron.one/api/v1/euri/embeddings\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"Authorization\": \"Bearer euri-78c66f791ec6c9b285bc2f92d2487d5c61f0bec3c10c01a8f89fb20aba1c5ae1\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"input\": text,\n",
    "        \"model\": \"text-embedding-3-small\"\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    data = response.json()\n",
    "    \n",
    "    embedding = np.array(data['data'][0]['embedding'])\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbe5ca7",
   "metadata": {},
   "source": [
    "# Next :\n",
    "\n",
    "- Now we have the data, we have the \"Chunks\", from this Chunks, I have to convert this entire data into a embedding by using, this \n",
    "- \"EURI API\" \n",
    "- Now here this one -> [ \"model\": \"text-embedding-3-small\" ] <- we are using the OpenAI based model, again this is the multilingual model.\n",
    "- that we are using, it support hundreds of languages, bydefault. When people was training this model obviously they have used the \n",
    "- multilingual dataset. \n",
    "- But, again one of the lighted & fasted embedding model and one of the low cost embedding model, we will be able to find out\n",
    "- in the market. There are again hundreds of other embedding model which is avaiable. \n",
    "\n",
    "- For Example :\n",
    "- Last class we have used from Hugging Face a \"Qwen3-Embedding-8B\" model. So, we can even go with that.\n",
    "\n",
    "- So, this model -> [ \"model\": \"text-embedding-3-small\" ] will going to give us a vector size return of ( 1536 ) is the dimension of the\n",
    "- one single vector, it is going to return.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f787be5a",
   "metadata": {},
   "source": [
    "# Next :\n",
    "\n",
    "- So, we have a \"Chunks\". we have a data, we will just run my generate embedding through this chunks, one by one & eventually,\n",
    "- I will be able to get my final embedding. \n",
    "\n",
    "- So, here , I am going to write the code, \n",
    "\n",
    "- code : [ \"for i in chunks\": ]\n",
    "- code : [ embedding = generate_embeddings(i) ] \n",
    "- code : [ print(embedding) ]\n",
    "- one by one lets do the Chunking & call this generate embedding function, -> [ embedding = generate_embeddings(i) ] <- pass the value \n",
    "- one by one, then try to print the emdedding. If will try to keep all embedding somewhere. \n",
    "- Now lets execute below code, As a outcome we can see that, it is like creating embedding for sentences,\n",
    "- for sentences No.1, Sentences No.2,Sentences No.3, Sentences No.4, Sentences No.5, Sentences No.6 etc...... \n",
    "- So, I have total 8 sentences. For all of those sentences it will able to create the embedding. So, it is able to create the numerics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28ff38bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01636502 -0.01330146  0.04360536 ... -0.00716132 -0.00025957\n",
      "  0.00777273]\n",
      "[-0.02828987 -0.00211549  0.03156904 ... -0.00953185 -0.01975844\n",
      " -0.00468256]\n",
      "[-0.01842189 -0.03448532  0.01168446 ... -0.0130075   0.00758591\n",
      "  0.02325387]\n",
      "[-0.00206819 -0.01932828  0.04296086 ... -0.033941    0.00859491\n",
      "  0.02564766]\n",
      "[ 0.01036524 -0.00490744  0.05058131 ... -0.0313709  -0.00884518\n",
      "  0.00210155]\n",
      "[-1.0751131e-02 -2.5514962e-02  3.7223070e-02 ... -1.7212370e-02\n",
      " -6.2131260e-05  4.0052750e-04]\n",
      "[ 0.00257804 -0.02860213  0.04973948 ...  0.00047777 -0.02138105\n",
      "  0.00431918]\n",
      "[ 0.04722496  0.00496422  0.04577735 ... -0.01690703 -0.07669617\n",
      "  0.00453403]\n"
     ]
    }
   ],
   "source": [
    "for i in chunks:\n",
    "    embedding = generate_embeddings(i)\n",
    "    print(embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17093de5",
   "metadata": {},
   "source": [
    "# Next :\n",
    "\n",
    "- Once I will be able to generate this chunks, now we can happily store this data into any of these databases.\n",
    "\n",
    "- [ 0.01636502 -0.01330146  0.04360536 ... -0.00716132 -0.00025957\n",
    "-  0.00777273]\n",
    "-[-0.02828987 -0.00211549  0.03156904 ... -0.00953185 -0.01975844\n",
    "- -0.00468256]\n",
    "-[-0.01842189 -0.03448532  0.01168446 ... -0.0130075   0.00758591\n",
    "-  0.02325387]  etc.......\n",
    "\n",
    "- So, whether it's an Quadrant, whether its an \"Pinecone\" wherther it's \"Chroma DB\", whether it's a \"Facebook AI Similarity Search\".\n",
    "- or any other databases, which is exist in my local or maybe which is available in some other platforms, depends, I will be able to \n",
    "- store this dataset in anywhere that I want. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643615d6",
   "metadata": {},
   "source": [
    "# Note : \"Enumerate\" ?\n",
    "- Lets understand what does \"Enumerate\" basically means inside python?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57256f8",
   "metadata": {},
   "source": [
    "- If we trying to pass a \"List\", so Enumeration will try to give a ID. \n",
    "- Lets suppose, if we have a List[] inside this list we have sentences -> [\"vijay\", \"Euron\", \"iNeuron\" ] <- this kind of data, we have\n",
    "- Let's try to write \"for i in \"Enumeration\" & print(i).\n",
    "- so, as a outcome, we can see over here, what it does basically. whatever data that we have inside the List[\"vijay\", \"Euron\", \"iNeuron\"],\n",
    "- it will take this data and it will try to return, indexes as well.(0,1,2).\n",
    "- This is what \"Enumerate\" does.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00619aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [\"vijay\", \"Euron\", \"iNeuron\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5b73e87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'vijay')\n",
      "(1, 'Euron')\n",
      "(2, 'iNeuron')\n"
     ]
    }
   ],
   "source": [
    "for i in enumerate(l):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf58367",
   "metadata": {},
   "source": [
    "# Next :\n",
    "\n",
    "- So here we just generate the embeddings, & I am just trying to like show the print, in a ideal way.\n",
    "- So, I have to basically create a complete embedding list and I have to create a Meta information, & then I have to go ahead and store,\n",
    "- those data. So, lets remodify these above piece of code. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c4d3c2",
   "metadata": {},
   "source": [
    "- Lets write code this way :\n",
    "- For idx & chunks in enumerate (chunks):\n",
    "- This is just extension of above code that we are writting. Above code we just written simple (\"Chunks\") & going one by one all of \n",
    "- this data, 8 sentences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebf6577",
   "metadata": {},
   "source": [
    "- But, here below code what we are trying to do is that, I am trying to \"Enumerate\". Enumerate basically does, \n",
    "- if we trying to pass a \"List\", so Enumeration will try to give a ID. \n",
    "- So lets suppose, we have List[] of the data inside a chunks, -> [ \"for idx,chunk in enumerate(chunks): \"] <-\n",
    "\n",
    "- For every list, it will going to return the (\"idx\"), as well as it is going to return the (\"Chunks\").\n",
    "- I am just trying to build my Meta information of the data. So that the region, that way I am trying to write.\n",
    "\n",
    "- So here it is going to return the (\"idx\"), it is going to return the (\"chunks\") & this is the property of \"Enumerate\".\n",
    "- This is the pattern -> indexes (0,1,2), this will going to return the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e873d4",
   "metadata": {},
   "source": [
    "# 1st Line code :\n",
    "\n",
    "- Here, I have created 2 blank list[], keep it blanks.\n",
    "- 1.Embedding List -> ( \"emb_list = []\" ) &\n",
    "- 2.Meta -> Meta[]\n",
    "\n",
    "# 1.Embedding List : \n",
    "- So, I have craeted Embedding_list[], where I will keep this all this embedding, which I am trying to produced.\n",
    "\n",
    "- [ 0.01636502 -0.01330146  0.04360536 ... -0.00716132 -0.00025957 0.00777273]\n",
    "- [-0.02828987 -0.00211549  0.03156904 ... -0.00953185 -0.01975844 -0.00468256]\n",
    "- [-0.01842189 -0.03448532  0.01168446 ... -0.0130075   0.00758591 0.02325387] etc,.....\n",
    "\n",
    "# 2.Meta List -> Meta[] :\n",
    "- Anpther is Meta List[], where I am going to keep may be ( \"IDs\" ), maybe the \"textual data\", \"Start Indexes\" & \"End Indexes\" all of \n",
    "- such information. May be I can try t store over there. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd2cfed",
   "metadata": {},
   "source": [
    "# 2nd line code : \n",
    "# Code : \n",
    "- for idx,chunk in enumerate(chunks):\n",
    "-     vec = generate_embeddings(chunk)\n",
    "\n",
    "- Here I will try to call a \"generate Embedding\" function ( ) & I am going to pass my \"chunks\", means I am going to pass my orginal data.\n",
    "- This one -> \"chunk\" 1st line code we can see & then it is going to retun me vector. \"Vector\" means a Numerical Value. \n",
    "\n",
    "- This one -> [ 0.01636502 -0.01330146  0.04360536 ... -0.00716132 -0.00025957 0.00777273]\n",
    "- [-0.02828987 -0.00211549  0.03156904 ... -0.00953185 -0.01975844 -0.00468256] etc,.....\n",
    "- It is going to return, \n",
    "\n",
    "# 3rd & 4th line code :\n",
    "- Code : vec = generate_embeddings(chunk)\n",
    "-    emb_list.append(vec)\n",
    "\n",
    "- So, here we have a vector, so here I am going to call embedding list.append what? append basically a Vector. SO, whatever it is trying \n",
    "- to generate try to append it.\n",
    "\n",
    "# 5th Line code: \n",
    "\n",
    "- Code : emb_list.append(vec.astype(\"float32\"))\n",
    "\n",
    "- Before convert it, lets try to property type, -> ( \"vec.astype\" ) & then so whatever we are trying to append it. Keep it inside the \n",
    "- ( \"Float32\" ) bit format. \"Numpy\" compatiable & along with Facebook AI similarity search compatiable. whenever we try to store it,\n",
    "- try to store at vector( \"float32\" ) bits."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e878b25d",
   "metadata": {},
   "source": [
    "- So this is going to basically keep on appending a data. It is going to append into \n",
    "- this, particular lists -> [ \"emb_list = [] | meta = []\" ] <- each and every data it is going to append."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73745b7a",
   "metadata": {},
   "source": [
    "# 6th Line Code :\n",
    "\n",
    "- Now, we have to create a [ \"Meta Information\" ]. SO, I am going to call \"meta.append\". \n",
    "- So, here I am trying to append -> ( \"id\" ) |  I am trying to append \"idx\" a Text original data, that we have.\n",
    "- This I am trying to tread as a \"Text\" data. So, it is going to be Meta Information.\n",
    "- Lets execute this.\n",
    "- Once, we execute this, it is going to create a vector & it is going to create and Meta information.\n",
    "- All this information will be avaiable inside the Embedding List, & it will be avaiable inside the Meta.\n",
    "\n",
    "- This below is the my final dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c4ed9f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_list = []\n",
    "meta = []\n",
    "for idx,chunk in enumerate(chunks):\n",
    "    vec = generate_embeddings(chunk)\n",
    "    emb_list.append(vec.astype(\"float32\"))\n",
    "\n",
    "    meta.append({\"id\": idx, \"text\": chunk})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708b1b58",
   "metadata": {},
   "source": [
    "- Now, if I will come & check my \"Embedding List\". \n",
    "- As a outcome, we can see I have all my data, avaiable in \"Float32\" format. And \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "90312870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 1.6688565e-02, -1.3501738e-02,  4.3314822e-02, ...,\n",
       "        -7.1801147e-03, -1.3807718e-05,  7.8434953e-03],\n",
       "       shape=(1536,), dtype=float32),\n",
       " array([-0.02824657, -0.00211815,  0.03152717, ..., -0.00951514,\n",
       "        -0.01976704, -0.00461856], shape=(1536,), dtype=float32),\n",
       " array([-0.0184284 , -0.03449934,  0.01166509, ..., -0.0130235 ,\n",
       "         0.00761142,  0.02324393], shape=(1536,), dtype=float32),\n",
       " array([-0.00206819, -0.01932828,  0.04296086, ..., -0.033941  ,\n",
       "         0.00859491,  0.02564766], shape=(1536,), dtype=float32),\n",
       " array([ 0.01036524, -0.00490744,  0.05058131, ..., -0.0313709 ,\n",
       "        -0.00884518,  0.00210155], shape=(1536,), dtype=float32),\n",
       " array([-0.01087056, -0.02549004,  0.03719883, ..., -0.01714737,\n",
       "        -0.00013056,  0.00037601], shape=(1536,), dtype=float32),\n",
       " array([ 0.00250368, -0.02838254,  0.04988758, ...,  0.00022234,\n",
       "        -0.02115859,  0.00431127], shape=(1536,), dtype=float32),\n",
       " array([ 0.04722496,  0.00496422,  0.04577735, ..., -0.01690703,\n",
       "        -0.07669617,  0.00453403], shape=(1536,), dtype=float32)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23634bce",
   "metadata": {},
   "source": [
    "- Now, If I will go ahead and check my \"Meta\", for all of this data.\n",
    "- As a outcome, we can see that, we have the \n",
    "- 1.( Id is 0 or id : 0 ) & this was the Text -> \"Making an Impact\\nHelping Millions of Students....\n",
    "- 2.( Id is 1 or id : 1 ) we have this text-> \"1.5 million students from 34+ countries, providing them...\n",
    "- 3.( Id is 2 or id : 2 ) we have this text-> 'backgrounds. They saw iNeuron as a lifeline—an opportunity.....\n",
    "\n",
    "- I am just trying to maintain a proper Meta data information about my dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fd0c4c",
   "metadata": {},
   "source": [
    "# Note : \n",
    "- So this was the the actual data -> 'id': 0, 'text': \"Making an Impact\\nHelping Millions of Students Succeed\\nSudhanshu's commitment\n",
    "- & this was the -> \n",
    "- [array([ 0.01636502, -0.01330146,  0.04360536, ..., -0.00716132, -0.00025957,  0.00777273], shape=(1536,), dtype=float32),\n",
    "- array([-0.02828987, -0.00211549,  0.03156905, ..., -0.00953185, -0.01975844, -0.00468256], shape=(1536,), dtype=float32)\n",
    "- this is basically respective conversion of those data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3102e297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 0,\n",
       "  'text': \"Making an Impact\\nHelping Millions of Students Succeed\\nSudhanshu's commitment to affordable education wasn't just a business strategy—it was his life's mission. Over the years, iNeuron has helped over 1.5 million students from 34+ countries, providing them with the skills they need to succeed in toda\"},\n",
       " {'id': 1,\n",
       "  'text': \"1.5 million students from 34+ countries, providing them with the skills they need to succeed in today's competitive job market.\\nMany of these students, like Sudhanshu himself, came from disadvantaged backgrounds. They saw iNeuron as a lifeline—an opportunity to rise above their circumstances.\\nIn 202\"},\n",
       " {'id': 2,\n",
       "  'text': 'backgrounds. They saw iNeuron as a lifeline—an opportunity to rise above their circumstances.\\nIn 2022, iNeuron was acquired by PhysicsWallah in a deal worth ₹250 crore. While this acquisition was a significant milestone, Sudhanshu remained focused on his mission. Even after the acquisition, iNeuron '},\n",
       " {'id': 3,\n",
       "  'text': \"gnificant milestone, Sudhanshu remained focused on his mission. Even after the acquisition, iNeuron continued to offer some of the most affordable and accessible tech courses in the world.\\n\\nThe Entrepreneur and Teacher: Sudhanshu's Dual Legacy\\nSudhanshu's journey isn't just one of entrepreneurial su\"},\n",
       " {'id': 4,\n",
       "  'text': \"reneur and Teacher: Sudhanshu's Dual Legacy\\nSudhanshu's journey isn't just one of entrepreneurial success; it's also a story of dedication to teaching. Throughout his career, he has remained a passionate educator, constantly looking for ways to empower others through knowledge. Whether teaching cour\"},\n",
       " {'id': 5,\n",
       "  'text': 'ate educator, constantly looking for ways to empower others through knowledge. Whether teaching courses in Big Data,\\nData Science, or programming, Sudhanshu has always sought to make complex subjects accessible to learners at all levels.\\n\\nHis commitment to affordable education has earned him the res'},\n",
       " {'id': 6,\n",
       "  'text': 'accessible to learners at all levels.\\n\\nHis commitment to affordable education has earned him the respect and admiration of countless students. Many credit Sudhanshu with changing their lives, helping them secure jobs, improve their skills, and break free from the limitations of their backgrounds.'},\n",
       " {'id': 7,\n",
       "  'text': 'them secure jobs, improve their skills, and break free from the limitations of their backgrounds.'}]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4606f705",
   "metadata": {},
   "source": [
    "# Next: \n",
    "- Now we have the Meta information, we have the data which is avaiable into a vector format. Now what we have to perform?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d81015f",
   "metadata": {},
   "source": [
    "# Next :\n",
    "\n",
    "- So, we have this dataset, if we go & check what is the datatype.\n",
    "- As we can see that this dataset is in the [ \"List\" ] format.\n",
    "- Because, I have created simple List and inside that list, I was trying to keep all these data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "529747aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(emb_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2e2e86",
   "metadata": {},
   "source": [
    "# Note :  \"Numpy Vector\" format\n",
    "\n",
    "- let's try to convert this dataset into a \"Numpy Vector\" format. So to do that I can try to call ( \"Numpy\".Vstack ) inside this,\n",
    "- I will try to pass this entire List or (emb_list). my variable is (\"xb\").\n",
    "- As a outcome, we can see that this entire vector converted into 2D vector, data inside the data. So we had list inside we had a \"Array\"\n",
    "- I am just trying to convert that into a \"Numpy compatiable\" format.\n",
    "- Now, Shape = (8, 1536), dtype=float32) <- means Now we have data of shape(8) into 8 rows & 1536 columns. \n",
    "- The length of the data is 1536 so that it is just trying to show us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3567dff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = np.vstack(emb_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b7ed2d",
   "metadata": {},
   "source": [
    "# Next :\n",
    "- Lets try to store this data into a variable called the (\"xb\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a446a14b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.6688565e-02, -1.3501738e-02,  4.3314822e-02, ...,\n",
       "        -7.1801147e-03, -1.3807718e-05,  7.8434953e-03],\n",
       "       [-2.8246567e-02, -2.1181451e-03,  3.1527173e-02, ...,\n",
       "        -9.5151449e-03, -1.9767037e-02, -4.6185642e-03],\n",
       "       [-1.8428400e-02, -3.4499343e-02,  1.1665091e-02, ...,\n",
       "        -1.3023502e-02,  7.6114177e-03,  2.3243934e-02],\n",
       "       ...,\n",
       "       [-1.0870559e-02, -2.5490044e-02,  3.7198830e-02, ...,\n",
       "        -1.7147368e-02, -1.3056057e-04,  3.7600618e-04],\n",
       "       [ 2.5036815e-03, -2.8382542e-02,  4.9887575e-02, ...,\n",
       "         2.2234007e-04, -2.1158595e-02,  4.3112719e-03],\n",
       "       [ 4.7224957e-02,  4.9642199e-03,  4.5777347e-02, ...,\n",
       "        -1.6907027e-02, -7.6696172e-02,  4.5340331e-03]],\n",
       "      shape=(8, 1536), dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2622bb",
   "metadata": {},
   "source": [
    "# Next :\n",
    "\n",
    "- Now we are able to send this data into the \"faiss\" Facebook AI Similarity search, so that we have to even called the Normilization.\n",
    "- So, fiass. try to call \"normalized_l2\" & I can try to pass my dataset, which is (\"xb\"). \n",
    "- so it will be able to normalized it. So here we are just trying to perform a normalization operation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d5580f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c6c9f843",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.normalize_L2(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8b7cd25",
   "metadata": {},
   "source": [
    "# Next :\n",
    "\n",
    "- Here variable = d & (xb).shape of [1]. as a outcome we can see that, (1536). Now that is going to be the dimension of \n",
    "- my vector database.\n",
    "- I am just trying to find out the dimension from the dataset. \n",
    "- For 1 dataset dimension is basically 1536 & I need this dimension, to define my total index dimension\n",
    "- inside the \"faiss\" Facebook AI Similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1832298b",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = xb.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a4e6e368",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e2d463",
   "metadata": {},
   "source": [
    "# Note : \n",
    "\n",
    "- Here what we can call is, there is something is called [\"IndexFlatIP\"] & inside that, we are going to pass a dimension (\"d\").\n",
    "- so, Basically this one -> (\"faiss\") <- is going to set a dimension for a particular data, \n",
    "- because inside this \"faiss\" Facebook AI Similarity search, I can store a data, any of my dimension, maybe 2,000+, 3,000+, 5,000+.\n",
    "- But the initially we have to define the dimension, ok fine what is the dimension of our dataset.\n",
    "- our data can be using different different models, & every models give us different different dimension.\n",
    "\n",
    "- 1.Code No.1 : \n",
    "\n",
    "- So, here I am going to set a dimension, & I am going to call it as a ( \"Index\" ) over here.\n",
    "- Just set a dimension, Here instead of (d), I can write the (1536), which is my dimension and model is going to return, we can even\n",
    "- do this one. this is completely fine. Now once we are able to create the \"indexes\".\n",
    "\n",
    "- 2.Code No.2 :\n",
    "\n",
    "- index.add() <- means index dot(.) add all the data at a time. Now once we are going to call the ( index.add ) & I will just going \n",
    "- to call my entire dataset this one -> (xb), which is again \"Numpy Array\" or \"Numpy format\" or Numpy compaitble data. It will be able to\n",
    "- store all of these data automatically. \n",
    "- This below code, this is just one single line where we are trying to add all the data. After this step my dataset has been added.\n",
    "- Now my dataset is store inside of \"faiss\" Facebook AI Similarity search.\n",
    "- So, step wise somebody ask, then this 2 line of code is enough or required to store a data. \n",
    "- To store the data, just create the indexes & call add, then done.\n",
    "- Its avaiable inside a memory, its avaiable inside the RAM, as of now. I can even try to create a Physical file and can try to store,\n",
    "- this indexes, & I can try to store the \"Meta Information\" again which is going to discussed now only."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc0e4bd",
   "metadata": {},
   "source": [
    "- Above codes we have discussed so far, we was just doing a \"Data Preprocessing\", preprocessing of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e07a1e61",
   "metadata": {},
   "source": [
    "- when I am trying to call \"index.add\", as of now everything in Memory only, it is not presisting my data into a hard disk into any \n",
    "- kind of physical file by the way, everything is avaiable into in Memory only. \n",
    "- To store any kind of data, only these 2 line of code is enough.\n",
    "- Create the \"Indexes\", whatever dimension that we have that we can get from the model, & then call add work is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87e84065",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatIP(1536)\n",
    "index.add(xb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0cc8c0",
   "metadata": {},
   "source": [
    "# Note :\n",
    "\n",
    "- As we discussed this is in Memory only, Let's suppose I would like to persist this entire indexes, in a file system. & apart from\n",
    "- that, I would like to persist my \"Meta Information\" because even this \"Meta information\" is aviable in Variable.\n",
    "- this one -> \n",
    "- {'id': 0, 'text': \"Making an Impact\\nHelping Millions ......},\n",
    "- {'id': 1, 'text': \"1.5 million students from 34+ countries,...\"},\n",
    "- {'id': 2,'text': 'backgrounds..... '},\n",
    "\n",
    "- This Meta data also technically avaiable in memory only. Once I will restart my computer, so else I going to close this \"VS code\".\n",
    "- After that I have to reexecute it, reload the entire things.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58afdad",
   "metadata": {},
   "source": [
    "- Let's go and try to store this entire information, into some file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0645d8",
   "metadata": {},
   "source": [
    "# Next : \n",
    "\n",
    "- Code No.1 : \n",
    "\n",
    "- So, here for \"Index\", I am going to create some \"variable\" -> [ \"index_path\" ].\n",
    "- Just try to store the \"index of Vijay data Faiss\". This is a file [\"index_vijay.faiss\" ] which I am going to create.\n",
    "- this is the name of the file -> (\"index_vijay\") & this is the -> (\"faiss\") <- file extension.\n",
    "\n",
    "- Now over here I am going to store all the indexes, all these vectors by the ways.\n",
    "\n",
    "- Code No.2 :\n",
    "\n",
    "- And \"meta\" information. Let's say variable is -> \"meta_path\" & I am going to create (\"meta_vijay.jsonl\") I am going to create &\n",
    "- I am going to keep all those \"meta data\" -> \n",
    "\n",
    "- means this one ->\n",
    "- 'id': 0, 'text': \"Making an Impact\\nHelping Millions.........\n",
    "- 'id': 1, 'text': \"1.5 million students from 34+ countries...........\n",
    "- 'id': 2, 'text': 'backgrounds. They saw iNeuron.............\n",
    "- 'id': 3,'text': \"gnificant milestone, Sudhanshu remained focused on his mission........... etc......\n",
    "\n",
    "- I can even attached date & time, if I want. I can even attached Geographical location, because with some data we can do that.\n",
    "- we can attached longitude and latitude. case by case it depend. What kind of problem we can going to solve."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87876dc7",
   "metadata": {},
   "source": [
    "# Next : \n",
    "\n",
    "- So, I have created 2 files, as of now file path. \n",
    "\n",
    "- 1.Code No.1 : index_path = \"index_vijay.faiss\" <- Here we are going to store the indexes &\n",
    "- 2.Code NO.2 : meta_path = \"meta_vijay.json1\" <- Here, I am going to store the Meta information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fafe6612",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_path = \"index_vijay.faiss\"\n",
    "meta_path = \"meta_vijay.json1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39258ca8",
   "metadata": {},
   "source": [
    "# Note : \n",
    "- Code No.1 :\n",
    "\n",
    "- Here, If we have to store (\"indexes\"), SO simply I can call \"Faiss\". write indexes, \n",
    "- This is my index vaiable -> [\"index\"] & this is my Index path -> [\"index_path\"], if we execute this, we can see our folder section,\n",
    "- new file has been created, name as -> [ \"index_vijay.faiss\" ]  a physical file, but we will not be able to read it out.\n",
    "- we can see our folder section new file name has been created but, if we click on this file -> (\"index_vijay.faiss\") & try to open it\n",
    "- we can't able to read that file, it will show my file but, we can't read it clearly, its a serialized file we will be able to find out.\n",
    "- Technically all this data is avaiable over here, into a bytecode, it is a serialized one.\n",
    "- [\"index_vijay.faiss\"] <- when we try to read the file, it tried to perform the deserialization.\n",
    "- So, here we are able to create a physical file.\n",
    "- so inside this physical file -> [\"index_vijay.faiss\"] <- I have all of these indxes, \n",
    "\n",
    "- This one check this line code -> [ emb_list ]\n",
    "- array([ 1.6688565e-02, -1.3501738e-02,  4.3314822e-02, ...,-7.1801147e-03, -1.3807718e-05,  7.8434953e-03]),\n",
    "- array([-0.02824689, -0.00209906,  0.03149973, ..., -0.0094944 ,-0.01982287, -0.00463947], shape=(1536,), dtype=float32),\n",
    "- array([-0.01842189, -0.03448532,  0.01168446, ..., -0.0130075 , 0.00758591,  0.02325387], shape=(1536,), dtype=float32),\n",
    "- array([-0.00206151, -0.01927499,  0.04288206, ..., -0.03394372, 0.0085956 ,  0.02564972], shape=(1536,), dtype=float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90d5967",
   "metadata": {},
   "source": [
    "- Now I am able to process my data  into hard disk, even if I am going to close my computer that is completely fine.\n",
    "- If I will open this file, -> [\"index_vijay.faiss\"] , I dnt have to go through all these lengthy process, which I have already done.\n",
    "- I can directly start from this particular file -> [\"index_vijay.faiss\"] & searching for the data. In a physical file, I can store\n",
    "- the data in a as a indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "afd4168d",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, index_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dcfe027",
   "metadata": {},
   "source": [
    "# Note : Meta information\n",
    "\n",
    "- Let's suppose, If I have store \"meta indexes\" information as well. I can write the code and store the information.Similar to above one\n",
    "- Lets try to import JSON, & operating system, I am going to import.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e1505a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json,os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dfdbc9",
   "metadata": {},
   "source": [
    "- Lets support, I am trying to store the \"Meta\" information, into a Physical file. \n",
    "\n",
    "- I can try to write ( with open ) meta_pathn \"w\" or write.\n",
    "- SO, we have this [\"meta\"]\n",
    "\n",
    "- this one -> \"meta\"\n",
    "- {'id': 0, 'text': \"Making an Impact\\nHelping Millions ......},\n",
    "- {'id': 1, 'text': \"1.5 million students from 34+ countries,...\"},\n",
    "- {'id': 2,'text': 'backgrounds..... '},\n",
    "\n",
    "- And, I am going to write this inside this file, now if I will execute this below code then, as a outcome, we see on our folder section,\n",
    "- it created a file name as -> [\"meta_vijay.json1\"] & if we double click and open it, we can see we can see the we have the same\n",
    "- \"Meta\" information, inside this file -> [\"meta_vijay.json1\"].\n",
    "- So, the \"meta\" Information I am able to store, Even my embedding, I am able to store it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "93f8b56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(meta_path, \"w\") as f:\n",
    "    for item in meta:\n",
    "        f.write(json.dumps(item) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c215ee7",
   "metadata": {},
   "source": [
    "# Note : creating physical file and able to store inside my physical hard-Disk\n",
    "\n",
    "- So, we are able to store both \"meta\" & \"vector\"\n",
    "\n",
    "- 1.\"index_vijay.faiss\"\n",
    "- 2.\"meta_vijay.json1\"\n",
    "\n",
    "- Now, we can see in the left side, folder or file section, these 2 has been created.\n",
    "- Now this is avaiable into my physical hark Disk. I am able to create a physical file.\n",
    "\n",
    "- Now, even after close my system and come back, I will be able to get the exact same thing.\n",
    "- So, storing is done. We are able to store our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1907ab04",
   "metadata": {},
   "source": [
    "# Note :  Search Operation :\n",
    "\n",
    "- Let's say I am going to write a something is like, [\"who is Vijay\"]? obviously from that information from that embedding.\n",
    "- it must give me some answer, it must give me some return. Now lets do the Search opeartion.\n",
    "- So, that I can write the query and it will give me in return some result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941bb542",
   "metadata": {},
   "source": [
    "# For Exampple : \n",
    "- 1.query = \"what was the mission of iNeuron?\"\n",
    "- 2.what has happened in 2022.\n",
    "- 3.May be who is \"Sudhanshu kumar?\n",
    "\n",
    "- So, we all know this information is already avaiable in that particular dataset.\n",
    "\n",
    "- Lets say: \n",
    "- Somebody write query, my query is this one -> [\"query = \"what was the mission of iNeuron?\"] <- 1st of all I will convert this query.\n",
    "- into a embeddings. So, I am going to follow the exact same thing done even before.\n",
    "\n",
    "- So, here I am going to generating Embedding for this particular query -> [\"query = \"what was the mission of iNeuron?\"].\n",
    "\n",
    "- Code No.2 : [\"generate_embeddings(query)\"]\n",
    "\n",
    "- once I am able to generate the embedding -> [\"generate_embeddings(query)\"] &\n",
    "\n",
    "- Code No.3 : [ \"astype(\"float32\")\"]\n",
    "\n",
    "- I wil convert this into a (\"float 32\"), because I have done that even before.\n",
    "\n",
    "- Code No.4 : [ reshape(1, -1) ]\n",
    "\n",
    "- Then I can try to call a reshape, so that then my dataset will be flat.\n",
    "\n",
    "- So, this will be going to my actual query -> [\"generate_embeddings(query)\"] & suppose I will name it as (\"q\")\n",
    "- This is my query and I am not able to convert into it into like a some Normalization format. \n",
    "\n",
    "- code No.5 : faiss.normalize_L2(q)\n",
    "\n",
    "- Now just try to call, ( \"faiss.normalize_L2(q)\" ) <- and try to pass the query, so that it will be able to calculate the norm.\n",
    "- In a denominator, we have the normalization. we can say this is equivalent distance. It will be able to calculate norm over here.\n",
    "\n",
    "- Code N0.6 :\n",
    "\n",
    "- Then I can try to call the \"index.search(q, 5), search wise I am trying to pass my query (\"q\") & I am trying to pass top K values.\n",
    "- means try to give me top ( 5 ) result or I can say give me top 3 result. That is also completely.\n",
    "- So, this is ( 5 ) basically a top \"K\".\n",
    "\n",
    "- whichever will be having, top 3 most like maxmimum similarity, So it is going to give me that result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b273a4",
   "metadata": {},
   "source": [
    "# Next :\n",
    "- If we execute this code, as a outcome, we can see it is able to give me some short result.\n",
    "- & ( \"Index.search\" ) wise, I am able to see some outcome, I am able to see some result.\n",
    "- index.search(q, 3) then, I will get Array No.[ 2,1,0 ] or I can try index.search(q, 5), then I will get something like -\n",
    "- Array No.2, Array No.1, & Array No.(0), Array No.3 & Array No.4 . This is the result which it is trying to give it to me.\n",
    "- Here is a score -> (array([[0.5985576 , 0.5481801 , 0.48705295, 0.3973113 , 0.20701069], in a decending order, it is trying to give me\n",
    "- the score. & in particular score, it is trying to give it to me an Arrays -> ([[2, 1, 0, 3, 4]]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "751b4c7a",
   "metadata": {},
   "source": [
    "# Below, what does this mean actually :\n",
    "\n",
    "- The 2nd score index is the best, this is what it actually means because similarity score is basically 0.5985576 or (0.59).\n",
    "- 1.So, Array No.[2] is the best, & \n",
    "- 2.then 2nd best Array is No.[1]. & \n",
    "- 3.then 3rd best Array is No.[0] &\n",
    "- 4.then 4th best Array is No.[3] &\n",
    "- 5.then 5th best Array is No.[4]\n",
    "\n",
    "- This is what it is trying to give it to me.\n",
    "- So, here we are able to get the result ->  array [2, 1, 0, 3, 4] & now we can do more in details coding and we can able to see \n",
    "- a result which will be Human readable.\n",
    "- As of now it is trying to give it to me \"Cosign\" similarity score & after that it is trying to give me respective indexes.\n",
    "- For this particular Array indexes, this is the similary score, which they are able to find out & \n",
    "- this is the arrangement in a decending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4e9299e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.59863365, 0.548123  , 0.48909983, 0.3973113 , 0.20664331]],\n",
       "       dtype=float32),\n",
       " array([[2, 1, 0, 3, 4]]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"what was the mission of iNeuron?\"\n",
    "q = generate_embeddings(query).astype(\"float32\").reshape(1, -1)\n",
    "faiss.normalize_L2(q)\n",
    "index.search(q, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775d572d",
   "metadata": {},
   "source": [
    "# Continue... 1:27:05 PM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fd4524",
   "metadata": {},
   "source": [
    "- Lets suppose, if we have to arrange this entire data, -> array[2, 1, 0, 3, 4] <- & we have to showcase, what is avaiable at\n",
    "- Array index No.[2] ?\n",
    "- Just left corner click on this file name -> [\"meta_vijay.json1\"] <- inside this we are able to see the Meta information \n",
    "- -> {\"id\": 2, \"text\": \"backgrounds. They saw iNeuron as a lifeline\\u2014an opportunity to rise above their circums...<- we can go & check\n",
    "- Even I can write python code, which I will be able to print, Becasue I have ID's even over here inside this -> [\"meta_vijay.json1\"] <-\n",
    "- So, Array No.[2], means \"ID:2\" {\"id\": 2, \"text\": \"backgrounds. They saw iNeuron as a lifeline\\u2014an opportunity ...... &\n",
    "- Array No.[4], means \"ID:4\" {\"id\": 4, \"text\": \"reneur and Teacher: Sudhanshu's Dual Legacy\\nSudhanshu's journey......\n",
    "- If we do a python coding and get to know, which line is mathching is with my line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b785370c",
   "metadata": {},
   "source": [
    "# Note :\n",
    "- what was my input -> [ query = \"what was the mission of iNeuron?\" ]\n",
    "- So, If I will go and check inside -> [\"meta_vijay.json1\"] <- we can see inside this \n",
    "- Line No.-> {\"id\": 2, \"text\": \"backgrounds. They saw iNeuron as a lifeline\\u2014an opportunity......} <- it is giving me accurate\n",
    "- result, although match score is [0.5985576 ] or (0.59),  which is closer to 1, but yes this is the -> [iNeuron?], most similar\n",
    "- search, I am able to get."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b828ad",
   "metadata": {},
   "source": [
    "# Next : Let's say I will asked [\"Who is Sudhanshu\"?]\n",
    "\n",
    "- As a outcome, we can see I am able to find top score is Array No.[6] index by the way.\n",
    "- If we open this file left corner -> [\"meta_vijay.json1\"] <- as we can see top score is \n",
    "- this line No.-> {\"id\": 6, \"text\": \"accessible to learners at all levels.\\n\\nHis commitment to....} is matching, which is mention.\n",
    "- We are able to get the desire result.\n",
    "\n",
    "- Now, I am able to store the information, after scoring the information. I am able to even \"Query\" the information. Its look like\n",
    "- that I am able to correct result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "74af9a0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.5228137 , 0.5069984 , 0.50195414, 0.48918533, 0.35711852]],\n",
       "       dtype=float32),\n",
       " array([[6, 5, 4, 3, 1]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"who is sudhanshu?\"\n",
    "q = generate_embeddings(query).astype(\"float32\").reshape(1, -1)\n",
    "faiss.normalize_L2(q)\n",
    "index.search(q, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6601e60",
   "metadata": {},
   "source": [
    "# Public Question :\n",
    "\n",
    "- Faiss normalized L2 query embedding, please explain why we applied?\n",
    "- Check my Notebook, I have written over there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60f1602",
   "metadata": {},
   "source": [
    "# Normalization ?\n",
    "\n",
    "- Let's discussed with respect to a \"Numpy\" Array.\n",
    "- Let's suppose I consider Numpy Array over here, inside this Array lets me take small, data lets say [4,5,6,7,8] this is something\n",
    "- inside \"Numpy\" Array.\n",
    "- I will keep inside variable name is -> [\"Test\"] <-\n",
    "- Now, I will convert this dataset into the \"float32\"\n",
    "- Now If I am going to execute, and I am going to call my \"Test\" data. As a outcome we can see that,\n",
    "- this is how my \"Test\" dataset looks like. -> [array([[4., 5., 6., 7., 8.]], dtype=float32)] <- after the data, \n",
    "- there will be a floating point number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e3c664c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([[4,5,6,7,8]], dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bae7fd63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[4., 5., 6., 7., 8.]], dtype=float32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730e85ed",
   "metadata": {},
   "source": [
    "# Here, If I am going to call a normalization !\n",
    "\n",
    "- Let's suppose -> np.linalg.norms(test) <- So it is giving me -> (13.784049) or 13.78 something like this, it is return to me.\n",
    "\n",
    "- See this is the differnece, I have taken one random array -> array([[4,5,6,7,8]] with respect to a vector.\n",
    "- If we try to find out the norms, means, if we are going to find out -> ||X|| (√4²,5²,6²,7²,8²) <- this is what it is going to written.\n",
    "- so this is something it is trying to return -> [ np.float32(13.784049) ]. & this is a Norm -> ||X||."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06d69a20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(13.784049)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6784bbb1",
   "metadata": {},
   "source": [
    "- Now, If I will going to call -> faiss.normalize_L2 & then If I am going to pass my \"test\" data.\n",
    "- What it will going to return, here it will try to pass. -> np.linalg.norm(test) <- As a outcome we can see it is closer to [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991f41f3",
   "metadata": {},
   "source": [
    "- Now, when I am going and try to apply a normalization with L2 & L2 is nothing but a regulation technique.\n",
    "- if we check above one -> here lenth is (13.784049) or 13 % with same data.\n",
    "- But, when I call the L2 on top of this this one -> \"normalize\" <- it has normalized each and every dataset to a unit value.\n",
    "- only [1].\n",
    "- So, that when I try to find out, a \"Cosine Similarity\". As we know the formula : \n",
    "- cosine = (a.b / ||a|| x ||b||). So what we are trying to say is that, lets normalizing, lets make it [1 & 1 ] this one -> ||a|| x ||b||\n",
    "- Technically, we are nullifying the entire denominators.SO our Cosin similarity score we will be able to find out, just by finding out,\n",
    "- dot(.) product & this is what this IP is doing.\n",
    "- So \"faiss index flatIP\" that we have called it from the \"Faiss\" databased.\n",
    "- Technically, we have called the libary called it \"index flatIP\". Technically it is trying to calculate the \"indexflatIP\".\n",
    "- IP or (inner product).\n",
    "- So, it is trying to calculate the \"Index Flat IP\". So, it just trying to calculate a (axb), & it do not support to calculate the\n",
    "- demoninator one. which is ||a|| x ||b||.\n",
    "- Region is simple, We have already Normalized it, means our denominator is always [1], for any vector.\n",
    "- Any vector we will going to paas, after normalization, if we are going to calculate the \"Norm\".\n",
    "- SO, after ( L2 ), if we are going to calculate the Norms, it will be always [1].\n",
    "- So, ||a|| x ||b|| or [ 1x 1 ] is going to be [1] only. \n",
    "- So, that is the region, we are try to calling a normalization over here. This is how this \"Index.FlatIP\" is going to work.\n",
    "- Technically, it will calculates just inner product IP or ( a x b ), it will going to calculate. \n",
    "- we are just conveting this denominator into [1], so that it is not going to effect it & (\"index flatIP\") bydefault,\n",
    "- it is going to calculate dot(.) of it.\n",
    "- This is how this libary has been build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "67c98e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.normalize_L2(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b123b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.99999994)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c90c9c8",
   "metadata": {},
   "source": [
    "# Python Mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db100d0a",
   "metadata": {},
   "source": [
    "- Now we can do a \"Python Mapping\", with respect to this \"Meta Information\" & then we will be able to get the Final result.\n",
    "- This is the one of the DataBase. but we have alot of others DataBase as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4daf8c",
   "metadata": {},
   "source": [
    "# Note : DataBases :\n",
    "\n",
    "- Go to Google and search for these DataBases :\n",
    "\n",
    "- 2.Qdrant DataBase URL : https://qdrant.tech/\n",
    "- 3..Pinecone DataBase  URL : https://www.pinecone.io/\n",
    "- 4.chromaDB DataBase URL : https://www.trychroma.com/\n",
    "- 5.weaviate DataBase URL : https://weaviate.io/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022dff29",
   "metadata": {},
   "source": [
    "# Note :\n",
    "\n",
    "- So, we have so many different different Databases, which is avaiable in a same segment, Some of these database going to provide you.\n",
    "- even an Manage Hosting, they have already hosted over the cloud platform, we don't have do anything, we have just go and do the \n",
    "- installation and start storing our dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
